{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e79e1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\genai\\autogen\\venv\\lib\\site-packages (1.65.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\genai\\autogen\\venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\genai\\autogen\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\genai\\autogen\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\genai\\autogen\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\genai\\autogen\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\genai\\autogen\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\genai\\autogen\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\genai\\autogen\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in d:\\genai\\autogen\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3fd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI  # Import OpenAI module or any relevant client class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d4db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861bdcd4-2fb2-4581-95f2-a9d95125425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll always have to start by creating a llm_config object to configure our agents\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-3.5-turbo\", \n",
    "    \"api_key\": openai.api_key\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a0933-9829-4538-87cf-b6bb9b8213b7",
   "metadata": {},
   "source": [
    "## Agents definition\n",
    "\n",
    "We will first start by defining our agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a3b868-1dcf-4b75-85d5-92bda36567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6eb1a-3729-423e-8f5a-7d3b226a114e",
   "metadata": {},
   "source": [
    "The first agent is our **Onboarding Personal Information Agent**. Its main goal is to get the name and location of the customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a04a7e0-9895-4aa1-92bb-2f6c2d1440cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding_personal_information_agent = ConversableAgent(\n",
    "    name=\"Onboarding_Personal_Information_Agent\",\n",
    "    system_message='''You are a helpful customer onboarding agent,\n",
    "    you work for a phone provider called ACME.\n",
    "    Your job is to gather the customer's name and location.\n",
    "    Do not ask for any other information, only ask about the customer's name and location.\n",
    "    After the customer gives you their name and location, repeat them \n",
    "    and thank the user, and ask the user to answer with TERMINATE to move on to describing their issue.\n",
    "    ''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258907ce-63b6-4857-b63a-3a8396c58eb8",
   "metadata": {},
   "source": [
    "The second agent is our **Onboarding Issue Agent**. Its main goal is to determine the issue the customer is facing with ACME's products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9940afb3-296a-4636-a68a-13f07b963845",
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding_issue_agent = ConversableAgent(\n",
    "    name=\"Onboarding_Issue_Agent\",\n",
    "    system_message='''You are a helpful customer onboarding agent,\n",
    "    you work for a phone provider called ACME,\n",
    "    you are here to help new customers get started with our product.\n",
    "    Your job is to gather the product the customer use and the issue they currently \n",
    "    have with the product,\n",
    "    Do not ask for other information.\n",
    "    After the customer describes their issue, repeat it and add\n",
    "    \"Please answer with 'TERMINATE' if I have correctly understood your issue.\" ''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e21a0-01a4-4519-a978-66453c8633f1",
   "metadata": {},
   "source": [
    "The third agent is our **Customer Engagement Agent**. Its main goal is to interact with the customer based on the previously gathered information until a human agent can start intertacting with the customer to solve their issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2390df11-5fb9-462a-b7c7-59a9490d89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_engagement_agent = ConversableAgent(\n",
    "    name=\"Customer_Engagement_Agent\",\n",
    "    system_message='''You are a helpful customer service agent.\n",
    "    Your job is to gather customer's preferences on news topics.\n",
    "    You are here to provide fun and useful information to the customer based on the user's\n",
    "    personal information and topic preferences.\n",
    "    This could include fun facts, jokes, or interesting stories.\n",
    "    Make sure to make it engaging and fun!\n",
    "    Return 'TERMINATE' when you are done.''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ff490-cbe0-4dad-ae8c-04bbd59d17eb",
   "metadata": {},
   "source": [
    "Finally, this time, we will also have to define a **customer proxy agent**. The customer proxy agent is a Conversable Agent that is not an LLM (`llm_config=False`), it will however have the `human_input_mode=\"ALWAYS\"` which means that you, the human, will play the role of this agent. You will be the customer through this agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9b203a-2478-41c3-a022-00016b18c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_proxy_agent = ConversableAgent(    #here the user will act like the agent\n",
    "    name=\"customer_proxy_agent\",\n",
    "    llm_config=False,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cfaddf-23c5-4916-bbc0-a667adae0c21",
   "metadata": {},
   "source": [
    "Alright, that's all for our agents!  \n",
    "\n",
    "The next step is now to orchestrate the chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14fc23-a5a8-4ac5-98ab-f887459eb704",
   "metadata": {},
   "source": [
    "## Chat orchestration\n",
    "\n",
    "We are now going to define how the chat will happen. This means that we will define in which order agents will interact and who'll interact with who when. To define this, we will use a list, that will contain several elements, each one corresponding to a chat. The chats will then happen in that specific order.\n",
    "\n",
    "Let's define a `chat` object and then the first chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca917e8-6ef2-4ab7-ab8e-7934c6ca9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = [] # This is going to be our list of chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd6872-aeb4-4e16-b1e3-aa771a695cc3",
   "metadata": {},
   "source": [
    "### 1. Onboarding Agent with Customer\n",
    "\n",
    "We will now define the first chat and add it to this list. The first chat will be between our first agent, the **Onboarding Personal Information Agent** and the **customer**, who is going to be us.\n",
    "\n",
    "The first message will be sent by the **Onboarding Personal Information Agent** and will be:\n",
    "> *Hello, I'm here to help you get started with our product. \n",
    "            Could you tell me your name?*\n",
    "\n",
    "#### Carrying data to the next chat\n",
    "\n",
    "In order to make the transition easier with the next agent, we are going to ask for a slightly different type of summary than we did before with this agent. We are going to request a summary generated by the LLM, but **we will specify that the summary should return the name and location of the customer in a JSON format:** `{'name': '', 'location': ''}`. This is a structured data format that can be easily read by another agent but **also by another app or protocol**. This shows how an LLM agent can be used to interact with other apps.\n",
    "\n",
    "Since we only want to transfer name and location to the next chat and we specifically specified how we want to transfer this data, we are going to add a new parameter, the `clear_history` to `True` which means that no data other than the one specified in the summary will be sent to the next chat. If we set it to `False` the agent from the next chat will be aware about the previous exchange with the user. We'll use that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00b17a8-2ff0-481f-a2dc-308d00acb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats.append(\n",
    "    {\n",
    "        \"sender\": onboarding_personal_information_agent,\n",
    "        \"recipient\": customer_proxy_agent,\n",
    "        \"message\": \n",
    "            \"Hello, I'm here to help you solve any issue you have with our products. \"\n",
    "            \"Could you tell me your name?\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_args\": {\n",
    "        \"summary_prompt\" : \"Return the customer information \"\n",
    "                             \"into a JSON object only: \"\n",
    "                             \"{'name': '', 'location': ''}\",\n",
    "        },\n",
    "        \"clear_history\" : True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a49e2c-0b46-4cfd-9402-956518ab6286",
   "metadata": {},
   "source": [
    "### 2. Issue Agent with Customer\n",
    "\n",
    "We will now define the second chat and add it to this list. The second chat will be between our second agent, the **Issue Agent** and the **customer**, who is going to be us again.\n",
    "\n",
    "The second message will be sent by the **Onboarding Personal Information Agent** and will be:\n",
    "> *Great! Could you tell me what issue you're currently having and with which product?*\n",
    "\n",
    "This time we're going to generate a summary, but we won't specify any format or specifc fata that must be carried over because we do not know what the exchange will yield specifically. We are also going to specify that we want to transfer the chat history to the next chat/agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff35a68-13a9-4f71-b3b3-c4f1a217380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats.append(\n",
    "    {\n",
    "        \"sender\": onboarding_issue_agent,\n",
    "        \"recipient\": customer_proxy_agent,\n",
    "        \"message\": \n",
    "                \"Great! Could you tell me what issue you're \"\n",
    "                \"currently having and with which product?\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"clear_history\" : False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976ec68-f2ce-4b00-b50a-77d172660b77",
   "metadata": {},
   "source": [
    "### 3. Customer Engagement Agent with Customer\n",
    "\n",
    "We will now define the third chat and add it to this list. The third chat will be between our third agent, the **Customer Engagement Agent** and the **customer**, who is going to be us again.\n",
    "\n",
    "The third message will be sent by the **Customer Engagement Agent** and will be:\n",
    "> *Can you tell me more about how you use our products or some topics interesting for you?*\n",
    "\n",
    "This time we're going to generate a summary so that the human agent can get this information in an easy and quick way when they take over the conversation, but we won't specify any format or specifc fata that must be carried over because we do not know what the exchange will yield specifically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1125501-e9dd-4e46-aca9-c38090fababb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sender': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4b4290>,\n",
       "  'recipient': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4f44a0>,\n",
       "  'message': \"Hello, I'm here to help you solve any issue you have with our products. Could you tell me your name?\",\n",
       "  'summary_method': 'reflection_with_llm',\n",
       "  'summary_args': {'summary_prompt': \"Return the customer information into a JSON object only: {'name': '', 'location': ''}\"},\n",
       "  'clear_history': True},\n",
       " {'sender': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4d7fe0>,\n",
       "  'recipient': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4f44a0>,\n",
       "  'message': \"Great! Could you tell me what issue you're currently having and with which product?\",\n",
       "  'summary_method': 'reflection_with_llm',\n",
       "  'clear_history': False},\n",
       " {'sender': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4f44a0>,\n",
       "  'recipient': <autogen.agentchat.conversable_agent.ConversableAgent at 0x15c7f4b4410>,\n",
       "  'message': \"While we're waiting for a human agent to take over and help you solve your issue, can you tell me more about how you use our products or some topics interesting for you?\",\n",
       "  'max_turns': 2,\n",
       "  'summary_method': 'reflection_with_llm'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats.append(\n",
    "        {\n",
    "        \"sender\": customer_proxy_agent,\n",
    "        \"recipient\": customer_engagement_agent,\n",
    "        \"message\": \"While we're waiting for a human agent to take over and help you solve \"\n",
    "        \"your issue, can you tell me more about how you use our products or some \"\n",
    "        \"topics interesting for you?\",\n",
    "        \"max_turns\": 2,\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "    }\n",
    ")\n",
    "chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5bc31-01ac-4745-b0dc-d40bbd5af48d",
   "metadata": {},
   "source": [
    "## Initiate the sequential chat\n",
    "\n",
    "Now that we finished orchestrating the chat, we can get it started!  \n",
    "For this to work, **you**, the customer, will have to roleplay as a customer that currently have an issue with your phone provider. Let's say that your internet does not work, or that you want more bandwidth, or that you want some help to setup port forwarding to play a game with some friends or some other thing. Have fun doing some roleplay!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1532b913-917d-4425-b2c7-aa8393900ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mOnboarding_Personal_Information_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Hello, I'm here to help you solve any issue you have with our products. Could you tell me your name?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GENAI\\AUTOGEN\\venv\\Lib\\site-packages\\autogen\\agentchat\\chat.py:57: UserWarning: Repetitive recipients detected: The chat history will be cleared by default if a recipient appears more than once. To retain the chat history, please set 'clear_history=False' in the configuration of the repeating agent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Personal_Information_Agent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOnboarding_Personal_Information_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Great, thank you for that. And may I know your current location?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Personal_Information_Agent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOnboarding_Personal_Information_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Great, thank you for providing your name as customer_proxy_agent and your location. To move forward, please type TERMINATE to describe the issue you're facing.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Personal_Information_Agent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOnboarding_Personal_Information_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Thank you for your patience. It seems there might be an issue with the information provided. Could you please confirm your name and location again?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Personal_Information_Agent):\n",
      "\n",
      "my loaction is india and my name is RAVI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOnboarding_Personal_Information_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Thank you for confirming your name as RAVI and your location as India. To proceed further, please type TERMINATE to describe the issue you're facing.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Personal_Information_Agent):\n",
      "\n",
      "terminate\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mOnboarding_Issue_Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Great! Could you tell me what issue you're currently having and with which product?\n",
      "Context: \n",
      "{'name': 'RAVI', 'location': 'India'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding_Issue_Agent):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen import initiate_chats\n",
    "\n",
    "chat_results = initiate_chats(chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e91cb1-96ef-4d9e-8079-a505dbc211c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "And that's all for the exchange! It could go on for lonjger, but remember that we specified a `max_turns` settings for each chat, there is no need to let this last for too long since we're just running examples.\n",
    "\n",
    "Now once the human agent is ready to take over the conversation, we can provide them with a summary of all the information necessary so that they can immediately get started with solving the issue of the customer using the following commads, as we previously explored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3155f466-c5ba-476e-a204-e2f099450d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{'name': 'RAVI', 'location': 'India'}\"\n",
      "'Terminate the onboarding process due to unspecified issue with the product.'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "for chat_result in chat_results:\n",
    "    #pprint.pprint(chat_result.chat_history) # We could also get the whole chat history with this command\n",
    "    pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfcc49-5e89-425a-a31e-55b9e09df2f7",
   "metadata": {},
   "source": [
    "The human can thus quickly learn about our customer, their name, location and issue and immediately get started on solving the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bf84f-1c91-4cdb-9227-637aefe41a31",
   "metadata": {},
   "source": [
    "## What is this for?\n",
    "\n",
    "This example shows you how you can integrate LLMs into an app in order to accomplish complex interactions with humans and gather specific information through natural communication. This is very powerful and is not something that was easy to do it before having LLMs. By having agents focused on a single task, we're able to ensure that the LLM won't diverge and remain focused on a simple task, greatly increasing chances that it accurately accomplishes it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
